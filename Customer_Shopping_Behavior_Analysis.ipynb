{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2c0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"customer_shopping_behavior.csv\")\n",
    "df.head()\n",
    "df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab91c2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the summary statastics\n",
    "df.describe()\n",
    "# By default it gives summary for Numerical Data only\n",
    "# To see numerical and categorical both use\n",
    "\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38764e0c",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "*   We begin by **checking missing values** in the dataset to understand data quality issues.\n",
    "\n",
    "* From the null value summary, we observe that the **review_rating column has 37 missing values**.\n",
    "\n",
    "* Before directly filling these values, it is important to **pause and think** about the right imputation strategy.\n",
    "\n",
    "* A common approach is to replace missing values with the **mean or median**.\n",
    "\n",
    "* Between the two, **median is preferred** over mean because:\n",
    "\n",
    "* **Mean is sensitive to outliers** and can distort the distribution.\n",
    "\n",
    "* **Median is robust to outliers** and better represents the central tendency.\n",
    "\n",
    "* One might be tempted to simply fill all missing ratings using the **overall median of the column.**\n",
    "\n",
    "* Although this method technically works, it is **not best practice in this scenario.**\n",
    "\n",
    "* Review ratings can **vary significantly across product categories.**\n",
    "\n",
    "* For example, the Clothing category may have a very different rating pattern compared to the Footwear category.\n",
    "\n",
    "* Using a single global median **would force all categories to share the same rating behavior**, which introduces **bias** into the dataset.\n",
    "\n",
    "* To avoid this, we adopt a more thoughtful and data-aware approach.\n",
    "\n",
    "* We **impute missing values** using the median review rating within each product category.\n",
    "\n",
    "**This ensures that:**\n",
    "\n",
    "* A missing rating for a **Clothing item is replaced with the median rating** of Clothing items.\n",
    "\n",
    "* It is **not influenced** by ratings from unrelated categories like Footwear.\n",
    "\n",
    "* This approach **preserves category-level characteristics, reduces bias, and results in higher-quality data** for downstream analysis and modeling.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84299e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now Lets Check the missing Values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b37bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We impute missing values using the median review rating within each product category\n",
    "\n",
    "#  That way if a clothing item is missing its review rating, it gets replaced with the median review rating of a clothing item, not a footwear item.\n",
    "\n",
    "df['Review Rating'] = df.groupby('Category')['Review Rating'].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3607c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857a01f",
   "metadata": {},
   "source": [
    "* Let’s first inspect the column names in the dataset.\n",
    "\n",
    "* We notice that the column names contain:\n",
    "\n",
    "  * A mix of uppercase and lowercase letters\n",
    "\n",
    "  * Spaces between words\n",
    "\n",
    "  * In some cases, inconsistent formatting\n",
    "\n",
    "* While this may look readable, it can create practical issues during analysis.\n",
    "\n",
    "* Column names with uppercase letters, spaces, or special characters require:\n",
    "\n",
    "  * Quotation marks in SQL queries\n",
    "\n",
    "  * Exact formatting recall in Python code\n",
    "\n",
    "* This increases the chances of syntax errors and slows down development.\n",
    "\n",
    "* To avoid these issues, it is considered a best practice to standardize column names.\n",
    "\n",
    "* We will convert all column names to snake_case.\n",
    "\n",
    "* Snake_case means:\n",
    "\n",
    "  * All letters are lowercase\n",
    "\n",
    "  * Words are separated using underscores (_)\n",
    "\n",
    "* For example:\n",
    "\n",
    "  Customer ID → customer_id\n",
    "\n",
    "* Standardizing column names improves:\n",
    "\n",
    "  * Code readability\n",
    "\n",
    "  * Consistency across Python and SQL\n",
    "\n",
    "  * Ease of querying and maintenance\n",
    "\n",
    "* This step ensures a clean, analysis-ready dataset and smoother downstream workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c55d0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.lower()\n",
    "df.columns = df.columns.str.replace(' ','_')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5300bcd8",
   "metadata": {},
   "source": [
    "* Now let’s take a look at the column names again.\n",
    "\n",
    "* We can see that most of the column names look clean and consistent.\n",
    "\n",
    "* However, the purchase_amount_usd column still contains unit information (usd) in the name.\n",
    "\n",
    "* While this is not wrong, it can be simplified for better readability and analysis.\n",
    "\n",
    "* Since the currency unit is already known or documented elsewhere, we can keep the column name concise.\n",
    "\n",
    "* Therefore, we will rename purchase_amount_(usd) to purchase_amount.\n",
    "\n",
    "* This improves clarity while keeping the naming convention consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8503326",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={'purchase_amount_(usd)':'purchase_amount'})\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deffdb30",
   "metadata": {},
   "source": [
    "* All column names are now standardized using snake_case, making the dataset clean and consistent.\n",
    "\n",
    "* With a well-prepared dataset, we can now move on to feature engineering.\n",
    "\n",
    "* Feature engineering helps us derive new insights by transforming existing variables into more meaningful features.\n",
    "\n",
    "* Age as a continuous variable is useful, but for segmentation and analysis, grouping ages can be more insightful.\n",
    "\n",
    "* Therefore, we will create a new categorical column called age_group.\n",
    "\n",
    "* This column will group customers into four meaningful age segments:\n",
    "\n",
    "    * Young Adult\n",
    "\n",
    "    * Adult\n",
    "\n",
    "    * Middle-aged\n",
    "\n",
    "    * Senior\n",
    "\n",
    "These age groups will help in:\n",
    "\n",
    "    * Customer segmentation\n",
    "\n",
    "    * Behavioral analysis\n",
    "\n",
    "    * Targeted marketing insights\n",
    "\n",
    "    * Easier visualization in dashboards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3af24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a column age_group\n",
    "labels = ['Young Adult', 'Adult', 'Middle-aged', 'Senior']\n",
    "df['age_group'] = pd.qcut(df['age'], q = 4, labels = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89bc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['age','age_group']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de72b257",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2165199a",
   "metadata": {},
   "source": [
    "* Next, we will create another engineered feature called purchase_frequency_days.\n",
    "\n",
    "* The dataset currently contains a column named frequency_of_purchases.\n",
    "\n",
    "* This column describes how often a customer shops, but it is stored in textual form (e.g., weekly, monthly, quarterly).\n",
    "\n",
    "* While text labels are easy to read, they are not ideal for numerical analysis.\n",
    "\n",
    "    * Working with text makes it difficult to:\n",
    "\n",
    "    * Compare customers quantitatively\n",
    "\n",
    "    * Calculate averages or trends\n",
    "\n",
    "    * Build metrics or models\n",
    "\n",
    "* To make the data more analysis-friendly, we will convert these textual frequencies into numerical values.\n",
    "\n",
    "* Specifically, we will map each frequency to the approximate number of days between purchases.\n",
    "\n",
    "* This results in a new numerical column called purchase_frequency_days.\n",
    "\n",
    "* By doing this, we transform qualitative information into quantitative data.\n",
    "\n",
    "* This makes it significantly easier to:\n",
    "\n",
    "    * Compare customer purchasing behavior\n",
    "\n",
    "    * Perform aggregations and calculations\n",
    "\n",
    "    * Use the feature in statistical analysis or modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f12f1e1",
   "metadata": {},
   "source": [
    "* To convert purchase frequency from text to numeric values, we first create a mapping dictionary.\n",
    "\n",
    "* This dictionary, called frequency_mapping, links each textual frequency to its equivalent number of days between purchases.\n",
    "\n",
    "    For example:\n",
    "\n",
    "    Weekly → 7 days\n",
    "\n",
    "    Fortnightly → 14 days\n",
    "\n",
    "    Monthly → 30 days\n",
    "\n",
    "    Quarterly → 90 days\n",
    "\n",
    "* Using a dictionary ensures:\n",
    "\n",
    "    Clear and transparent logic\n",
    "\n",
    "    Easy modification if business assumptions change\n",
    "\n",
    "* Once the mapping is defined, we use the map() function.\n",
    "\n",
    "* The map() function replaces each textual value in the frequency_of_purchases column with the corresponding numeric value from the dictionary.\n",
    "\n",
    "* The result is a new numerical feature called purchase_frequency_days.\n",
    "\n",
    "* This transformation makes the data quantitative, consistent, and easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaa506d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create column purchase_frequency_days\n",
    "# convert textual frequency in numbers\n",
    "\n",
    "frequency_mapping = {\n",
    "    'Fortnightly' : 14,\n",
    "    'Weekly' : 7,\n",
    "    'Monthly' : 30,\n",
    "    'Quarterly': 90,\n",
    "    'Bi-weekly' : 14,\n",
    "    'Annually': 365,\n",
    "    'Every 3 Months' : 90\n",
    "}\n",
    "\n",
    "df['purchase_frequency_days'] = df['frequency_of_purchases'].map(frequency_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891126ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['purchase_frequency_days', 'frequency_of_purchases']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498c0c9",
   "metadata": {},
   "source": [
    "* After converting purchase frequency into numeric days, we now have a much clearer view of customer purchasing behavior.\n",
    "\n",
    "* Instead of vague terms like weekly or monthly, we can now analyze the exact number of days between purchases, which is extremely valuable for:\n",
    "\n",
    "    Customer segmentation\n",
    "\n",
    "    Loyalty analysis\n",
    "\n",
    "    Behavioral comparisons\n",
    "\n",
    "* Next, let’s examine two related columns:\n",
    "\n",
    "    discount_applied\n",
    "\n",
    "    promo_code_used\n",
    "\n",
    "* At first glance, these two columns may appear to represent the same information.\n",
    "\n",
    "* It’s easy to assume that if a promo code was used, a discount must have been applied.\n",
    "\n",
    "* However, in real-world business scenarios:\n",
    "\n",
    "    Discounts can be applied without promo codes\n",
    "\n",
    "    Examples include:\n",
    "\n",
    "    Automatic seasonal sales\n",
    "\n",
    "    Member-exclusive discounts\n",
    "\n",
    "    System-applied price reductions\n",
    "\n",
    "* This raises important data questions:\n",
    "\n",
    "    Do we actually need both columns?\n",
    "\n",
    "    Do they always contain the same values?\n",
    "\n",
    "    Is one column redundant?\n",
    "\n",
    "* Before making any assumptions or dropping columns, we should validate this using data.\n",
    "\n",
    "* Therefore, the next step is to analyze and compare these two columns to understand their relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8fbe2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['discount_applied', 'promo_code_used']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbca198",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df['discount_applied'] == df['promo_code_used']).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4590eeb",
   "metadata": {},
   "source": [
    "* We compare the discount_applied and promo_code_used columns using an equality check to see if they contain the same values.\n",
    "\n",
    "* By applying .all() to the comparison, we verify whether every row matches across both columns.\n",
    "\n",
    "* The result returns True, which means both columns carry exactly the same information.\n",
    "\n",
    "* Since promo_code_used does not add any additional insight, it is redundant.\n",
    "\n",
    "* Keeping redundant columns can increase complexity without improving analysis.\n",
    "\n",
    "* Therefore, we remove the promo_code_used column from the dataset using df.drop() to keep the data clean and streamlined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67829a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('promo_code_used', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71817103",
   "metadata": {},
   "source": [
    "* Now promo code use will be dropped. Let's check. \n",
    "\n",
    "* And you can see there is no promo code used. We only have discount applied.\n",
    "\n",
    "All right. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fee98",
   "metadata": {},
   "source": [
    "**I tried following steps in PostgreSQL and it worked, though I am feeling more confident using My SQL so I am proceed with that. Following are the step to follow in PostgreSQL**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25365e3a",
   "metadata": {},
   "source": [
    "\n",
    "* Now that we have completed initial data exploration and feature engineering, the next step is to move the dataset into PostgreSQL for deeper analysis.\n",
    "\n",
    "* Using SQL allows us to:\n",
    "\n",
    "        Run complex queries\n",
    "\n",
    "        Analyze customer behavior more efficiently\n",
    "\n",
    "        Answer business questions in a structured, scalable way\n",
    "\n",
    "* Before loading the data, we first need a database to store our table.\n",
    "\n",
    "* In pgAdmin, we create a new database by:\n",
    "\n",
    "        Right-clicking on Databases\n",
    "\n",
    "        Selecting Create → Database\n",
    "\n",
    "        Naming the database (e.g., customer_behavior)\n",
    "\n",
    "* Once the database is created and saved, we return to the Jupyter Notebook.\n",
    "\n",
    "* The next step is to connect Jupyter Notebook with PostgreSQL.\n",
    "\n",
    "* To enable this connection, we need two libraries:\n",
    "\n",
    "        psycopg – to establish a connection with PostgreSQL\n",
    "\n",
    "        SQLAlchemy – to manage database connections and load data efficiently\n",
    "\n",
    "* Therefore, we install both libraries using pip before proceeding with the database connection and data loading steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883e65f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2-binary sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c43cce",
   "metadata": {},
   "source": [
    "* Now that the database is ready, the next step is to connect our Jupyter Notebook to PostgreSQL.\n",
    "\n",
    "* To establish this connection, we need a few credentials:\n",
    "\n",
    "        Username\n",
    "\n",
    "        Password\n",
    "\n",
    "        Host\n",
    "\n",
    "        Port\n",
    "\n",
    "        Database name (the one we just created)\n",
    "\n",
    "* Once the connection is established, we can load our Pandas DataFrame into PostgreSQL as a new table.\n",
    "\n",
    "* The table can be named anything; for now, we’ll name it customer.\n",
    "\n",
    "* If you’re unsure about the credentials:\n",
    "\n",
    "        The host is usually localhost\n",
    "\n",
    "        The default port for PostgreSQL is 5432\n",
    "\n",
    "        The password is the same one used to log in to pgAdmin\n",
    "\n",
    "* To verify the remaining details:\n",
    "\n",
    "        Open pgAdmin\n",
    "\n",
    "        Right-click on PostgreSQL\n",
    "\n",
    "        Select Properties\n",
    "\n",
    "        Go to the Connection tab\n",
    "\n",
    "        Here, you can find the host name, port, and username\n",
    "\n",
    "* Once these details are confirmed, we can safely proceed to connect Python with PostgreSQL and push the data into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e94f456",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "# # Step 1: Connect to PostgreSQL\n",
    "# # Replace Placeholders with your actual details\n",
    "\n",
    "# username = \"postgres\"       # default user\n",
    "# password = \"Pass@123\"      # the password you set during installation\n",
    "# host = \"localhost\"          # if running locally\n",
    "# port = \"5432\"               # default PostgreSQL port\n",
    "# database = \"mydatabase\"      # The database you created in pgAdmin\n",
    "\n",
    "# engine = create_engine(f\"postgresql+psycopg2://{username}:{password}@{host}:{port}/{database}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f782cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Important: I add this step because in my password: Pass@123 contains \"@\"\" symbol and it generates error To resolve this \n",
    "# # IMPORTANT NOTE: Connecting to PostgreSQL with SQLAlchemy\n",
    "# # - If your password contains special characters like @, #, :, / etc.,\n",
    "# #   you MUST escape it properly using urllib.parse.quote_plus()\n",
    "# # - Without escaping, SQLAlchemy misinterprets the connection string\n",
    "# #   (e.g. password \"Pass@123\" → thinks host is \"23@localhost\")\n",
    "# # - Solution used here: quote_plus() – safest & recommended method\n",
    "\n",
    "# Escape the password (critical step!)\n",
    "# escaped_password = quote_plus(password)\n",
    "\n",
    "# engine = create_engine(\n",
    "#     f\"postgresql+psycopg2://{username}:{escaped_password}@{host}:{port}/{database}\"\n",
    "# )\n",
    "\n",
    "# from urllib.parse import quote_plus\n",
    "\n",
    "# password = \"Pass@123\"\n",
    "# engine = create_engine(\n",
    "#     f\"postgresql+psycopg2://postgres:{quote_plus(password)}@localhost:5432/customer_behavior\"\n",
    "# )\n",
    "\n",
    "# df.to_sql(\"customer\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee07249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 2: Load DataFrame into PostgreSQL\n",
    "# table_name = \"customer\"     # Choose any table name\n",
    "# df.to_sql(table_name, engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# print(f\"Data Successfully loaded into table '{table_name}' in database '{database}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee14602",
   "metadata": {},
   "source": [
    "**Code for MySQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8264a782",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pymysql sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b0fd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from urllib.parse import quote_plus\n",
    "\n",
    "username = \"root\"\n",
    "password = \"Pass@123\"\n",
    "host = \"localhost\"\n",
    "port = 3306\n",
    "database = \"mydatabase\"\n",
    "\n",
    "# Safest way\n",
    "engine = create_engine(\n",
    "    f\"mysql+pymysql://{username}:{quote_plus(password)}@{host}:{port}/{database}\"\n",
    ")\n",
    "\n",
    "print(\"Engine created successfully!\")\n",
    "# Now you can use engine with pandas, etc.\n",
    "# df.to_sql(..., con=engine, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61f4010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write DataFrame to My SQL\n",
    "# Before running this you need to create database in mysql\n",
    "\n",
    "table_name = 'mytable'      # Choose any table name\n",
    "df.to_sql(table_name, engine, if_exists = \"replace\", index = False)\n",
    "\n",
    "print(f\"Data Successfully loaded into table '{table_name}' in database '{database}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7600c40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read back sample\n",
    "pd.read_sql(\"select * from mytable limit 5;\" , engine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
